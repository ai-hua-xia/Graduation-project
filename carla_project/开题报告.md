# 本科毕业设计（论文）开题报告

**学号**：2022300385
**姓名**：王怡恒
**指导教师**：高君宇
**报告题目**：第一人称视角下的可交互3D城市街景世界模型研究
**论文类型**：工程设计类
**报告日期**：2026年1月12日

---

## 一、选题背景、意义及依据

### 1.1 研究背景

随着人工智能技术的飞速发展，自动驾驶和智能交通系统已成为当前计算机视觉和机器人领域最具挑战性和前景的研究方向之一。自动驾驶车辆需要在复杂多变的城市环境中进行实时决策，这不仅需要精确的感知能力，更需要对环境变化的预测能力。传统的自动驾驶系统通常采用模块化的设计方法，将感知、规划和控制分离为独立的模块，这种方法虽然便于调试和理解，但往往难以处理模块间的信息传递损失和延迟问题。

近年来，"世界模型"（World Model）的概念逐渐成为人工智能领域的研究热点。世界模型是一种能够学习环境动态规律的神经网络模型，它可以根据当前的观测和采取的动作来预测环境的未来状态。这种模型最早由Ha和Schmidhuber在2018年提出，他们证明了智能体可以完全在其学习到的世界模型的"梦境"中进行训练，而无需与真实环境交互。这一发现为强化学习和自动驾驶领域带来了革命性的思路转变。

在自动驾驶场景中，世界模型的价值尤为突出。首先，自动驾驶系统的训练需要大量的驾驶数据，而收集真实世界的驾驶数据不仅成本高昂，还存在安全风险。世界模型可以生成逼真的虚拟驾驶场景，为算法训练提供几乎无限的数据支持。其次，世界模型可以用于预测车辆行为的后果，帮助规划系统做出更安全、更合理的决策。最后，世界模型还可以用于驾驶员培训、事故分析和交通仿真等应用场景。

当前，以生成式人工智能为代表的大模型技术取得了突破性进展。从文本生成的GPT系列到图像生成的Stable Diffusion，再到视频生成的Sora，生成式AI展现出了强大的创造能力。特别是OpenAI于2024年发布的Sora模型，首次展示了AI生成长时间、高质量、物理逼真视频的能力，引发了学术界和工业界对视频世界模型的广泛关注。Waymo、NVIDIA等公司也相继发布了针对自动驾驶场景的世界模型研究成果，如GAIA-1、DriveDreamer等，这些工作证明了世界模型在自动驾驶领域具有巨大的应用潜力。

### 1.2 研究意义

本课题旨在研究第一人称视角下的可交互3D城市街景世界模型，其研究意义主要体现在以下几个方面：

**（1）理论意义**

本研究将深入探索动作条件化视频生成的机制，研究如何将驾驶动作（如转向、加速）有效地融入视频生成过程。这涉及到条件生成模型、序列建模和视觉表示学习等多个深度学习核心问题。通过本研究，可以加深对动作-视觉映射关系的理解，为通用世界模型的构建提供理论支撑。

此外，本研究还将探索离散视觉表示与自回归生成范式在动态场景中的适用性。VQ-VAE等离散表示方法已在图像和视频压缩领域取得成功，但在动作条件化生成任务中的表现尚未得到充分研究。本课题将为该领域提供实验依据和方法论参考。

**（2）实践意义**

从实践角度看，本研究具有直接的工程应用价值。首先，可交互的城市街景世界模型可以为自动驾驶系统提供高质量的仿真环境，减少对真实道路测试的依赖，降低开发成本和安全风险。其次，该模型可以用于生成多样化的训练数据，包括极端天气、罕见场景等难以在现实中大量收集的数据，从而提升自动驾驶算法的鲁棒性。

在更广泛的应用层面，城市街景世界模型还可以应用于：城市规划的可视化预演、驾驶员培训的虚拟现实系统、交通事故的场景重建与分析、以及游戏和娱乐产业中的城市环境生成。这些应用展示了世界模型技术在多个领域的巨大潜力。

**（3）教育意义**

本课题涉及深度学习、计算机视觉、生成模型等多个前沿技术领域，是一个综合性很强的研究项目。通过本课题的研究，可以系统地学习和实践VQ-VAE、Transformer、FiLM调制等先进的深度学习技术，培养独立开展科研工作的能力，为后续的研究生阶段学习或工业界工作打下坚实基础。

### 1.3 选题依据

本课题的选题依据主要包括以下几个方面：

**（1）技术可行性**

从技术角度看，本课题所需的关键技术已相对成熟。VQ-VAE（向量量化变分自编码器）已被证明是一种有效的视觉压缩方法，能够将高维图像压缩为低维的离散token序列。Transformer架构在序列建模任务中表现出色，已被广泛应用于自然语言处理和计算机视觉领域。FiLM（Feature-wise Linear Modulation）是一种成熟的条件调制技术，可以有效地将条件信息融入神经网络的中间表示。这些技术的成熟为本课题的实施提供了坚实的技术基础。

**（2）数据和工具支持**

CARLA（Car Learning to Act）是一个开源的自动驾驶仿真平台，提供了高度逼真的城市环境渲染和丰富的传感器模拟。相比于其他仿真平台，CARLA具有以下优势：真实的3D城市场景（包含建筑物、车辆、行人等丰富元素）、可控的交通流和天气条件、精确的车辆物理模型、以及便捷的Python API接口。这些特性使得CARLA成为采集高质量城市街景数据的理想平台。

**（3）研究问题的明确性**

在前期的预研工作中，我们发现在简单道路场景（如MetaDrive平台生成的开放道路）中，转向动作对视觉的影响非常微弱（光流仅2-3像素），导致世界模型难以学习到动作与视觉变化之间的对应关系。这一问题明确了本课题的核心挑战：如何在富有视觉变化的城市场景中构建能够准确响应驾驶动作的世界模型。CARLA的城市场景提供了丰富的视觉参考物（建筑物、路标、其他车辆等），转向时会产生明显的视差变化，这为解决上述问题提供了可行的研究路径。

**（4）计算资源保障**

本课题依托实验室的高性能计算平台，配备2块NVIDIA H200 NVL GPU（各150GB显存），能够支持大规模深度学习模型的训练。同时，PyTorch 2.4.0等现代深度学习框架提供了混合精度训练（BF16）等技术，可以显著提升训练效率和模型规模。这些计算资源的保障使得本课题能够训练约200M参数的大型世界模型。

---

## 二、国内外研究现状

### 2.1 世界模型研究进展

世界模型的概念最早可追溯到1989年Schmidhuber提出的"预测性学习"思想，但真正引起广泛关注是在2018年Ha和Schmidhuber发表的"World Models"论文。该工作使用VAE（变分自编码器）对观测进行压缩，用RNN（循环神经网络）学习环境动态，并在压缩的潜在空间中训练策略网络。他们在CarRacing和VizDoom等游戏环境中展示了完全在"梦境"中训练智能体的可行性。

此后，世界模型领域涌现出大量研究成果。Dreamer系列工作（Hafner等人，2019-2023）提出了基于模型的强化学习框架，通过学习世界模型来进行高效的策略优化。DreamerV3在多个复杂任务上达到了state-of-the-art性能，证明了世界模型在通用任务中的有效性。

在自动驾驶领域，世界模型的研究更为活跃。GAIA-1（Wayve，2023）是首个针对自动驾驶场景的大规模生成式世界模型，使用了90亿参数的扩散模型，能够根据文本描述和驾驶动作生成逼真的驾驶视频。DriveDreamer（Li等人，2023）提出了一种基于扩散模型的驾驶场景生成方法，支持多视角和多帧的一致性生成。UniSim（Yang等人，2023）构建了一个统一的仿真框架，可以从真实视频中学习物理世界的动态规律。

2024年初，OpenAI发布的Sora模型引发了视频生成领域的革命。Sora使用扩散Transformer（DiT）架构，能够生成长达一分钟的高质量视频，展现出对物理规律的初步理解能力。虽然Sora并非专门针对自动驾驶场景设计，但其技术路线为自动驾驶世界模型的发展提供了重要参考。

### 2.2 VQ-VAE与离散视觉表示

VQ-VAE（Vector Quantized Variational AutoEncoder）由van den Oord等人于2017年提出，是一种将连续图像表示转换为离散token序列的方法。与传统VAE使用连续潜在空间不同，VQ-VAE维护一个可学习的码本（codebook），将编码器输出的连续向量量化为码本中最近的离散向量。这种离散表示具有以下优势：

（1）信息压缩效率高：一张256×256的图像可以被压缩为16×16的token序列，每个token仅需10比特（假设1024个码本向量），总共约2.5KB，实现了约1000倍的压缩比。

（2）便于自回归建模：离散token序列可以直接使用Transformer等自回归模型进行建模和生成，无需处理连续分布的复杂性。

（3）多模态融合友好：离散表示可以方便地与文本等其他模态进行统一建模，为多模态世界模型奠定基础。

后续工作如VQ-VAE-2（Razavi等人，2019）通过层次化的量化策略进一步提升了图像质量。VQGAN（Esser等人，2021）引入对抗训练和感知损失，显著改善了重建图像的视觉质量。Magvit（Yu等人，2023）专门针对视频数据设计了高效的VQ编码器，在视频压缩和生成任务上取得了优异性能。

### 2.3 Transformer在视频生成中的应用

Transformer架构最初由Vaswani等人于2017年为机器翻译任务设计，但其强大的序列建模能力使其迅速扩展到计算机视觉领域。ViT（Vision Transformer，Dosovitskiy等人，2020）证明了将图像划分为patch并使用Transformer处理的有效性。此后，Transformer在图像生成（如DALL-E）、视频理解（如ViViT）等任务中均取得了显著成功。

在视频生成领域，基于Transformer的自回归方法与基于扩散模型的方法形成了两条主要技术路线。自回归方法（如VideoGPT）将视频帧序列token化后，使用Transformer逐token预测生成；扩散方法（如Video Diffusion Models）则在连续空间中通过去噪过程生成视频帧。两种方法各有优劣：自回归方法训练稳定、易于控制，但生成速度较慢；扩散方法生成质量高，但需要大量采样步骤。

在动作条件化生成方面，FiLM（Feature-wise Linear Modulation，Perez等人，2018）是一种被广泛采用的条件注入机制。FiLM通过学习条件变量到仿射变换参数的映射，对网络中间特征进行调制，能够有效地将动作等条件信息融入生成过程。这种方法已被成功应用于视觉问答、机器人控制等多个领域。

### 2.4 自动驾驶仿真平台

自动驾驶仿真平台是开发和测试自动驾驶算法的重要工具。主流的仿真平台包括：

（1）CARLA：由Intel和丰田合作开发的开源仿真器，基于Unreal Engine 4构建，提供高度逼真的城市环境和丰富的传感器模拟。CARLA支持多种地图、天气条件和交通参与者，是学术界最广泛使用的自动驾驶仿真平台之一。

（2）MetaDrive：由加州大学伯克利分校开发的轻量级仿真器，专注于大规模程序化环境生成。MetaDrive的优势在于运行速度快、环境多样性高，但其场景以简单道路为主，缺乏复杂的城市元素。

（3）NVIDIA Drive Sim：工业级仿真平台，提供物理精确的车辆模型和传感器仿真，但需要商业许可。

（4）AirSim：微软开源的仿真平台，支持无人机和自动驾驶汽车，基于Unreal Engine构建。

本课题选择CARLA作为数据采集平台，主要考虑其开源性、城市场景的丰富性以及与深度学习框架的良好兼容性。

### 2.5 国内研究现状

国内在世界模型和自动驾驶领域也开展了大量研究工作。清华大学、北京大学、上海交通大学等高校在视频生成、自动驾驶等方向持续产出高水平成果。华为、百度、蔚来等企业也在积极布局世界模型技术，用于自动驾驶系统的仿真和测试。

特别值得一提的是，腾讯发布的HunyuanVideo和阿里的EMU系列工作在视频生成领域展现了国内在大规模生成模型方面的技术实力。这些工作为本课题的技术选型和方法设计提供了重要参考。

---

## 三、课题研究目标、研究内容、研究方法及关键技术

### 3.1 研究目标

本课题的总体研究目标是：设计并实现一个基于深度学习的第一人称视角城市街景世界模型，该模型能够根据驾驶动作（转向、油门）预测并生成未来的视觉画面，实现对驾驶场景的可交互仿真。

具体研究目标包括：

**（1）数据采集目标**
- 使用CARLA仿真器采集高质量的城市街景驾驶数据
- 采集规模：100个episode，每个episode 100-200帧，共计10000-20000帧
- 数据多样性：覆盖5种城市地图、4种天气条件
- 动作分布：通过转向优先采集策略，确保转向数据占比达到70%

**（2）VQ-VAE模型目标**
- 实现高质量的图像压缩与重建，重建PSNR > 25dB
- 有效的离散表示学习，码本利用率 > 85%
- 将256×256图像压缩为16×16的token网格（256个token）

**（3）世界模型目标**
- 准确的动作条件化预测，模型能够根据输入动作生成对应的视觉变化
- 良好的时间连贯性，生成帧之间SSIM > 0.92
- 实时生成能力，推理帧率 > 15 FPS
- 支持长序列生成，能够稳定生成200帧以上的视频

**（4）系统集成目标**
- 完整的端到端生成流水线：初始帧 + 动作序列 → 视频
- 支持键盘实时控制的交互式演示
- 提供评估指标和可视化工具

### 3.2 研究内容

本课题的研究内容主要包括以下四个部分：

**（1）基于CARLA的城市街景数据采集**

首先，需要搭建CARLA仿真环境并实现自动化的数据采集流程。数据采集的核心挑战在于如何获取富有动作响应性的驾驶数据。为此，将设计"转向优先"的采集策略：

- 场景选择：选取Town01-Town05共5种城市地图，包含T型路口、曲线道路、复杂路网等多种道路类型
- 天气设置：在ClearNoon（晴天中午）、CloudyNoon（多云）、WetNoon（雨天）、ClearSunset（黄昏）等4种天气条件下采集
- 驾驶策略：实现autopilot驾驶，并通过路口强制转向等方式提高转向数据的比例
- 数据记录：同步记录第一人称RGB图像（256×256）和对应的驾驶动作（转向角、油门值）

数据采集还需要考虑数据质量控制，包括：
- 过滤低速帧（速度 < 2m/s的静止画面）
- 检测并处理碰撞事件
- 确保图像和动作数据的时间同步

**（2）VQ-VAE视觉压缩模型**

VQ-VAE是本课题的基础模块，负责将高维图像压缩为低维的离散token序列。研究内容包括：

模型架构设计：
- 编码器：采用4层下采样卷积网络，将256×256图像编码为16×16的特征图
- 向量量化层：维护1024个256维的码本向量，使用最近邻查找进行量化
- 解码器：采用4层上采样转置卷积网络，从量化表示重建原始图像
- 残差块：在编码器和解码器中使用ResidualBlock增强特征提取能力

训练策略设计：
- 损失函数：重建损失（MSE） + VQ损失（码本对齐损失 + commitment损失）
- 混合精度训练：使用BF16精度加速训练并节省显存
- 学习率调度：使用AdamW优化器，初始学习率2e-4
- 训练轮次：100个epoch

评估指标：
- 重建质量：PSNR、SSIM
- 码本利用率：统计被使用的码本向量比例
- 感知质量：LPIPS

**（3）基于Transformer的世界模型**

世界模型是本课题的核心，负责根据历史帧和动作预测未来帧。研究内容包括：

模型架构设计：
- 输入处理：将4帧历史token序列（每帧256个token）和4个动作向量作为输入
- Token嵌入：将离散token映射为512维连续向量
- 位置编码：使用可学习的位置编码表示token的空间位置
- 动作编码器：使用3层MLP将动作序列编码为hidden_dim维的条件向量
- FiLMed Transformer：16层带FiLM调制的Transformer层，每层包含多头自注意力、FiLM调制和前馈网络
- 输出投影：预测下一帧256个位置的token分布（每个位置1024类）

FiLM调制机制：
FiLM层通过动作embedding生成缩放参数gamma和偏移参数beta，对特征进行仿射变换：
```
output = gamma * hidden + beta
```
这种机制使得动作信息能够在每一层细粒度地影响特征表示，比简单的条件拼接更加有效。

训练策略设计：
- 损失函数：交叉熵损失 + 时间平滑损失
- 课程学习：平滑权重从0逐步增加到0.02（60轮预热期）
- 动作自适应平滑：大动作时降低平滑约束，允许更大的视觉变化
- 混合精度训练：使用BF16精度
- 学习率：5e-5（大模型使用较小学习率）
- 训练轮次：300个epoch

时间平滑损失的设计是本课题的技术创新点之一：
```
smooth_loss = KL(P_t || P_{t+1}) * exp(-beta * action_magnitude)
```
该损失通过KL散度约束相邻帧预测分布的相似性，同时根据动作幅度自适应调整约束强度。

**（4）视频生成与评估**

研究内容的最后一部分是完整的视频生成流程和评估体系：

视频生成流程：
1. 从数据集中选取初始的4帧作为context
2. 循环执行：使用世界模型预测下一帧token → 使用VQ-VAE解码为RGB图像 → 更新token buffer
3. 将生成的图像序列合成为视频文件

交互式演示：
- 支持键盘实时输入动作（W/A/S/D控制）
- 实时显示生成的视频帧
- 可视化当前动作和模型状态

评估指标体系：
- 视觉质量：FID（Fréchet Inception Distance）、LPIPS
- 时间连贯性：连续帧SSIM、光流一致性
- 动作响应性：动作变化时的视觉变化幅度
- 长期稳定性：长序列生成的质量衰减曲线

### 3.3 研究方法

本课题采用"理论分析-模型设计-实验验证-迭代优化"的研究方法：

**（1）理论分析**
在模型设计之前，首先对现有的世界模型、VQ-VAE和Transformer技术进行深入的理论分析，理解各技术的优势和局限性。特别关注动作条件化生成的理论基础，分析FiLM等条件注入机制的工作原理。

**（2）模型设计**
基于理论分析的结论，设计适合城市街景生成任务的模型架构。在设计过程中，充分考虑任务的特殊性（如动作响应性要求）和计算资源约束（如显存限制），做出合理的架构选择和超参数设定。

**（3）实验验证**
实现所设计的模型，并在采集的CARLA数据集上进行训练和评估。通过定量指标和定性观察来验证模型的有效性，识别存在的问题和改进方向。

**（4）迭代优化**
根据实验结果，对模型架构、训练策略和超参数进行迭代优化。这一过程可能包括：调整网络深度和宽度、改进损失函数设计、优化数据增强策略等。

### 3.4 关键技术

**（1）VQ-VAE视觉量化技术**

VQ-VAE的核心是向量量化层，其工作原理如下：

给定编码器输出的连续特征z，向量量化层从码本E = {e_1, ..., e_K}中找到与z最近的码本向量：
```
z_q = e_k, where k = argmin_j ||z - e_j||_2
```

由于argmin操作不可微，训练时使用"Straight-through Estimator"：前向传播时使用量化后的z_q，反向传播时将梯度直接传递给z。损失函数包含三部分：
```
L = L_recon + ||sg[z] - e||_2^2 + β||z - sg[e]||_2^2
```
其中sg表示stop gradient，第二项将码本向量拉向编码器输出，第三项（commitment loss）防止编码器输出与码本偏离过远。

**（2）FiLM条件调制技术**

FiLM通过学习条件变量到仿射变换参数的映射来实现条件注入：
```
FiLM(x|c) = γ(c) ⊙ x + β(c)
```
其中γ和β由神经网络从条件c预测得到。

在本课题中，条件c是编码后的动作序列。FiLM的优势在于：
- 不改变特征的维度，计算开销小
- 可以在网络的每一层进行调制，实现细粒度控制
- 初始化为恒等变换，有利于训练稳定性

**（3）课程学习与自适应平滑**

课程学习策略用于渐进式引入时间平滑约束：
- 训练初期（0-60轮）：平滑权重为0，模型自由学习动作-视觉映射
- 训练后期（60-300轮）：平滑权重线性增加到0.02，逐步引入时间连贯性约束

动作自适应平滑通过动作幅度调整约束强度：
```
weight = exp(-β * ||action||)
```
大幅度转向时允许更大的视觉变化，小幅度动作时要求更高的连贯性。

**（4）混合精度训练技术**

为了在有限的GPU显存下训练大规模模型，本课题采用混合精度训练技术。具体使用BF16（Brain Floating Point 16）数据类型，其特点是：
- 保持与FP32相同的指数位数（8位），数值范围相同
- 减少尾数位数（7位 vs 23位），精度略有损失
- 显存占用减少约一半，训练速度提升约1.5倍

PyTorch的torch.cuda.amp模块提供了便捷的混合精度训练接口，可以自动管理精度转换和梯度缩放。

---

## 四、论文所遇到的困难和问题、拟采取的解决措施及预期达到的目标

### 4.1 当前进展与已解决的问题

截至开题报告提交时，本课题已完成以下工作：

**（1）开发环境搭建（已完成）**
- 完成CARLA 0.9.15仿真环境的安装和配置
- 配置PyTorch 2.4.0 + CUDA 12.4深度学习环境
- 完成2×H200 NVL GPU的多卡训练配置

**（2）数据采集（已完成）**
- 完成CARLA数据采集脚本的开发
- 采集了100个episode的驾驶数据，共计约10000帧
- 数据覆盖5种城市地图和4种天气条件
- 通过转向优先策略确保了转向数据的充足性

**（3）VQ-VAE训练（已完成）**
- 完成VQ-VAE模型的实现和训练
- 训练40个epoch后模型收敛，重建质量良好
- 成功将图像压缩为16×16的token序列
- 完成token序列的导出

**（4）世界模型训练（进行中）**
- 完成世界模型架构的实现
- 当前已训练262个epoch（计划300个epoch）
- 训练过程稳定，CE损失持续下降
- 课程学习策略按计划执行，平滑权重已达到目标值0.02

### 4.2 当前面临的困难和问题

尽管项目进展顺利，但在研究过程中仍然面临以下困难和问题：

**问题一：动作响应性与时间连贯性的平衡**

描述：在世界模型训练中发现，如果过度强调动作响应性（允许模型产生大幅度的视觉变化），生成的视频容易出现闪烁和抖动；如果过度强调时间连贯性（限制相邻帧的差异），模型可能会忽略动作输入，生成几乎不变的画面。如何在两者之间取得平衡是一个关键挑战。

影响：这一问题直接影响生成视频的可用性。过度闪烁会降低视觉体验，而缺乏动作响应则失去了"可交互"的核心价值。

**问题二：长序列生成的质量衰减**

描述：在长序列生成（>100帧）时，由于误差的累积，生成质量可能逐渐下降。具体表现为画面逐渐模糊、出现异常伪影、或偏离合理的场景结构。

影响：限制了世界模型在实际应用中的可用序列长度，影响其作为仿真工具的价值。

**问题三：罕见场景的泛化能力**

描述：虽然采集了多种地图和天气的数据，但某些罕见场景（如复杂路口、极端天气）的数据量相对有限，模型在这些场景下的泛化能力可能不足。

影响：降低了模型在实际应用中的鲁棒性，限制了其在多样化场景中的表现。

**问题四：推理速度的优化**

描述：当前的200M参数模型虽然生成质量较高，但推理速度可能无法满足实时交互的要求。在单GPU上，需要确保推理帧率达到15FPS以上。

影响：影响交互式演示的流畅性和用户体验。

### 4.3 拟采取的解决措施

针对上述问题，拟采取以下解决措施：

**解决措施一：优化课程学习和自适应平滑策略**

针对动作响应性与时间连贯性的平衡问题，计划进一步优化训练策略：

(1) 细化课程学习的阶段划分，将预热期从60轮延长到100轮，使模型有更充分的时间学习动作映射
(2) 调整动作自适应平滑的beta参数，根据实验结果找到最优的权重衰减曲线
(3) 考虑引入感知损失（如LPIPS），在像素级约束之外增加语义级的一致性约束
(4) 实验不同的平滑权重终值（0.01-0.05），选择最佳的平衡点

预期在剩余的训练轮次中完成这些调优工作。

**解决措施二：引入技术手段缓解误差累积**

针对长序列生成的质量衰减问题，计划采取以下措施：

(1) 降低推理时的采样温度（temperature），使预测更加确定性
(2) 使用top-k采样，过滤低概率的token选择，减少噪声引入
(3) 定期使用ground truth帧重置context buffer（teacher forcing风格），研究最优的重置频率
(4) 考虑在训练时增加长序列的curriculum，逐步增加预测步数

这些技术将在世界模型训练完成后的评估和优化阶段实施。

**解决措施三：数据增强与模型正则化**

针对罕见场景泛化能力不足的问题，计划：

(1) 在现有数据上进行数据增强，包括颜色抖动、随机裁剪、水平翻转等
(2) 增加dropout比例，提高模型的泛化能力
(3) 如时间允许，采集更多的罕见场景数据补充训练集
(4) 使用早停策略，防止模型过拟合到训练集

**解决措施四：模型压缩与推理优化**

针对推理速度的问题，计划采取以下优化措施：

(1) 使用torch.compile()进行图编译优化，提升推理效率
(2) 评估是否可以减少Transformer层数或注意力头数，在保持质量的前提下加速
(3) 使用KV-cache优化自注意力计算
(4) 考虑模型量化（INT8）进一步加速推理

### 4.4 预期达到的目标

通过解决上述问题，本课题预期达到以下目标：

**定量目标**

| 指标 | 目标值 |
|------|--------|
| VQ-VAE重建PSNR | > 25 dB |
| VQ-VAE码本利用率 | > 85% |
| 世界模型CE损失 | < 3.0 |
| 生成帧连续SSIM | > 0.92 |
| 动作响应延迟 | < 2帧 |
| 推理帧率（单GPU） | > 15 FPS |
| 稳定生成长度 | > 200帧 |

**定性目标**

(1) 生成的视频在视觉上自然连贯，无明显的闪烁或伪影
(2) 转向动作能够产生明显且合理的视觉变化（如建筑物的视差移动）
(3) 不同天气和地图条件下均能生成合理的场景
(4) 交互式演示流畅，响应及时
(5) 代码和模型具有良好的可复现性和文档完整性

**学术目标**

(1) 完成一份高质量的毕业论文，系统阐述世界模型的技术方案和实验结果
(2) 如有条件，将研究成果整理后投稿至计算机视觉或机器人领域的学术会议

---

## 五、论文进度安排

本课题的论文工作计划分为以下几个阶段进行：

### 第一阶段：文献调研与方案设计（2025年10月-2025年12月）【已完成】

**主要工作内容**：
- 调研世界模型、VQ-VAE、Transformer等领域的国内外研究现状
- 分析现有自动驾驶仿真和视频生成方法的优缺点
- 确定技术路线，设计系统整体架构
- 完成开发环境的搭建（CARLA、PyTorch等）
- 编写数据采集脚本并进行初步测试

**完成情况**：
该阶段工作已按计划完成。通过文献调研，明确了采用VQ-VAE + Transformer + FiLM的技术路线；完成了CARLA仿真环境和深度学习环境的配置；开发并测试了数据采集脚本。

### 第二阶段：数据采集与预处理（2026年1月-2026年1月中旬）【已完成】

**主要工作内容**：
- 使用CARLA采集大规模城市街景驾驶数据
- 采集目标：100个episode，涵盖多种地图和天气
- 实现转向优先的采集策略
- 对采集数据进行质量检查和预处理
- 整理数据集目录结构，编写数据加载代码

**完成情况**：
该阶段工作已完成。成功采集了100个episode的高质量数据，数据覆盖了Town01-Town05共5种城市地图和4种天气条件。转向数据比例达到了预期的70%左右。数据预处理和加载代码已开发完成。

### 第三阶段：模型开发与训练（2026年1月中旬-2026年2月中旬）【进行中】

**主要工作内容**：
- 实现VQ-VAE模型，完成训练和评估
- 使用训练好的VQ-VAE导出token序列
- 实现世界模型（Transformer + FiLM）
- 完成世界模型的训练，包括课程学习策略
- 对训练过程进行监控和记录

**当前进展**：
VQ-VAE训练已完成，token导出已完成。世界模型当前已训练262轮，预计在1月底前完成300轮训练。训练过程稳定，各项指标符合预期。

**后续工作**：
- 完成剩余的38轮训练
- 根据训练结果进行超参数微调
- 保存最终模型权重

### 第四阶段：实验评估与优化（2026年2月中旬-2026年3月中旬）【待开始】

**主要工作内容**：
- 实现视频生成流水线和交互式演示系统
- 对模型进行全面的定量评估（FID、SSIM、LPIPS等）
- 进行消融实验，验证各技术组件的有效性
- 针对发现的问题进行模型优化
- 测试不同场景下的生成效果

**具体计划**：
- 第1周：完成视频生成流水线开发
- 第2周：实现评估指标计算和交互式演示
- 第3周：进行消融实验和对比实验
- 第4周：根据实验结果进行模型优化

### 第五阶段：论文撰写与答辩准备（2026年3月中旬-2026年5月）【待开始】

**主要工作内容**：
- 整理实验数据和结果
- 撰写毕业论文初稿
- 根据导师意见修改论文
- 准备答辩PPT和演示
- 进行答辩预演

**具体计划**：
- 3月中旬-4月中旬：完成论文初稿
- 4月中旬-4月底：论文修改
- 5月初：完成论文定稿
- 5月中旬：准备答辩

### 进度风险分析

**潜在风险**：
1. 世界模型训练时间可能超出预期
2. 模型效果可能不达预期，需要额外的调优时间
3. 论文撰写可能占用比预期更多的时间

**应对措施**：
1. 保持灵活的时间安排，预留缓冲时间
2. 与导师保持密切沟通，及时调整研究方向
3. 边实验边记录，减少后期论文撰写的工作量
4. 如时间紧张，适当简化某些非核心实验

---

## 六、参考文献

[1] Ha D, Schmidhuber J. World models[J]. arXiv preprint arXiv:1803.10122, 2018.

[2] Van Den Oord A, Vinyals O. Neural discrete representation learning[C]//Advances in neural information processing systems. 2017: 6306-6315.

[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.

[4] Perez E, Strub F, De Vries H, et al. Film: Visual reasoning with a general conditioning layer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2018, 32(1).

[5] Hafner D, Lillicrap T, Ba J, et al. Dream to control: Learning behaviors by latent imagination[C]//International Conference on Learning Representations. 2019.

[6] Hafner D, Lillicrap T, Norouzi M, et al. Mastering atari with discrete world models[C]//International Conference on Learning Representations. 2021.

[7] Hafner D, Pasukonis J, Ba J, et al. Mastering diverse domains through world models[J]. arXiv preprint arXiv:2301.04104, 2023.

[8] Hu A, Russell L, Yeo H, et al. GAIA-1: A generative world model for autonomous driving[J]. arXiv preprint arXiv:2309.17080, 2023.

[9] Wang Z, Zheng L, Liu Y, et al. DriveDreamer: Towards real-world-driven world models for autonomous driving[C]//European Conference on Computer Vision. 2024.

[10] Yang S, Wang G, Xu Z, et al. UniSim: A neural closed-loop sensor simulator[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 1389-1399.

[11] Razavi A, Van den Oord A, Vinyals O. Generating diverse high-fidelity images with vq-vae-2[C]//Advances in neural information processing systems. 2019: 14866-14876.

[12] Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 12873-12883.

[13] Yu L, Cheng Y, Sohn K, et al. Magvit: Masked generative video transformer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10459-10469.

[14] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]//International Conference on Learning Representations. 2020.

[15] Yan W, Zhang Y, Abbeel P, et al. VideoGPT: Video generation using VQ-VAE and transformers[J]. arXiv preprint arXiv:2104.10157, 2021.

[16] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[C]//Advances in Neural Information Processing Systems. 2020, 33: 6840-6851.

[17] Ho J, Salimans T, Gritsenko A, et al. Video diffusion models[J]. arXiv preprint arXiv:2204.03458, 2022.

[18] Peebles W, Xie S. Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4195-4205.

[19] Brooks T, Peebles B, Holmes C, et al. Video generation models as world simulators[R]. OpenAI Technical Report, 2024.

[20] Dosovitskiy A, Ros G, Codevilla F, et al. CARLA: An open urban driving simulator[C]//Conference on robot learning. PMLR, 2017: 1-16.

[21] Li Q, Peng Z, Feng L, et al. MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, 45(3): 3461-3475.

[22] Wang Z, Bovik A C, Sheikh H R, et al. Image quality assessment: from error visibility to structural similarity[J]. IEEE transactions on image processing, 2004, 13(4): 600-612.

[23] Heusel M, Ramsauer H, Unterthiner T, et al. GANs trained by a two time-scale update rule converge to a local nash equilibrium[C]//Advances in neural information processing systems. 2017: 6626-6637.

[24] Zhang R, Isola P, Efros A A, et al. The unreasonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 586-595.

[25] Micikevicius P, Narang S, Alben J, et al. Mixed precision training[C]//International Conference on Learning Representations. 2018.

---

**备注**：本开题报告基于CARLA世界模型项目的实际开发进展撰写，截至2026年1月12日，项目已完成数据采集、VQ-VAE训练和世界模型训练的主体工作（262/300轮），处于优化和评估阶段的前期。
