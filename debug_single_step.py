import torch
import cv2
import numpy as np
from train_vqvae_256 import VQVAE, DEVICE
from train_world_model import WorldModelGPT, BLOCK_SIZE

# é…ç½®
VQVAE_PATH = "checkpoints_vqvae_256/vqvae_256_ep99.pth"
WORLD_MODEL_PATH = "checkpoints_world_model/world_model_ep10.pth" # ç”¨é‚£ä¸ª Loss=2.0 å·¦å³çš„
DATA_PATH = "dataset_v2_complex/tokens_actions_vqvae_16x16.npz"

def test_teacher_forcing():
    # 1. åŠ è½½æ‰€æœ‰ä¸œè¥¿
    vqvae = VQVAE().to(DEVICE)
    vqvae.load_state_dict(torch.load(VQVAE_PATH, map_location=DEVICE)["model"])
    
    gpt = WorldModelGPT().to(DEVICE)
    gpt.load_state_dict(torch.load(WORLD_MODEL_PATH, map_location=DEVICE)["model"])
    gpt.eval()
    
    data = np.load(DATA_PATH)
    tokens = torch.from_numpy(data['tokens']).long().to(DEVICE) # (N, 16, 16)
    actions = torch.from_numpy(data['actions']).float().to(DEVICE) # (N, 2)
    
    # 2. é€‰å–ä¸€æ®µçœŸå®çš„è¿ç»­æ•°æ® (æ¯”å¦‚ç¬¬ 500-504 å¸§)
    start_idx = 500
    seq_len = 3
    
    # å†å²: 0, 1, 2, 3
    input_tokens = tokens[start_idx : start_idx+seq_len].reshape(1, seq_len, 256) 
    input_actions = actions[start_idx : start_idx+seq_len].unsqueeze(0)
    
    # çœŸå®ç›®æ ‡: ç¬¬ 4 å¸§ (å³ start_idx + 4)
    target_tokens = tokens[start_idx + seq_len] 
    
    print("ğŸ¤– GPT is predicting the NEXT frame based on REAL history...")
    
    # 3. è®© GPT é¢„æµ‹ä¸‹ä¸€å¸§ (Teacher Forcing)
    with torch.no_grad():
        # æ„é€ è¾“å…¥ (B, seq, 256)
        # è¿˜éœ€è¦åŠ ä¸Šè¿™ä¸€æ­¥çš„åŠ¨ä½œï¼Œç”¨æ¥é¢„æµ‹è¿™ä¸€æ­¥çš„å›¾
        # æˆ‘ä»¬è¿™é‡Œç®€åŒ–ï¼Œç›´æ¥çœ‹å®ƒèƒ½ä¸èƒ½æ ¹æ®å†å²é¢„æµ‹æœªæ¥
        # æ³¨æ„ï¼šè®­ç»ƒæ—¶è¾“å…¥æ˜¯ (img, act)ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª img
        # æˆ‘ä»¬æ‰‹åŠ¨æ„é€ ä¸€ä¸ª dummy è¾“å…¥æ¥è§¦å‘é¢„æµ‹
        
        # ä¸ºäº†é¢„æµ‹ç¬¬ 5 å¸§ï¼Œæˆ‘ä»¬éœ€è¦è¾“å…¥å‰ 4 å¸§ + ç¬¬ 5 å¸§çš„åŠ¨ä½œ
        next_action = actions[start_idx + seq_len].view(1, 1, 2)
        
        # æ­¤æ—¶è¿˜æ²¡æœ‰ç¬¬ 5 å¸§çš„å›¾åƒï¼Œæˆ‘ä»¬ç”¨å…¨0å ä½ï¼Œè®© GPT å¡«ç©º
        dummy_next_token = torch.zeros((1, 1, 256), dtype=torch.long).to(DEVICE)
        
        full_input_tokens = torch.cat([input_tokens, dummy_next_token], dim=1) # seq=5
        full_input_actions = torch.cat([input_actions, next_action], dim=1)    # seq=5
        
        # é€åƒç´ ç”Ÿæˆç¬¬ 5 å¸§
        generated_tokens = []
        for i in range(256):
            logits, _ = gpt(full_input_tokens, full_input_actions)
            
            # å–å‡ºå¯¹åº”ä½ç½®çš„ logit
            # å†å²é•¿åº¦ seq_len=4. å¯¹åº” flattened index æ˜¯ 4 * 257 + i - 1
            idx_in_flat = seq_len * 257 + i - 1
            if idx_in_flat >= logits.shape[1]: idx_in_flat = logits.shape[1]-1

            next_logit = logits[:, idx_in_flat, :]
            
            # è´ªå©ªé‡‡æ · (å–æ¦‚ç‡æœ€å¤§çš„ï¼Œä¸éšæœº) çœ‹å®ƒåˆ°åº•å­¦åˆ°äº†ä»€ä¹ˆ
            token_id = torch.argmax(next_logit, dim=-1)
            
            full_input_tokens[0, -1, i] = token_id # å¡«å›å»
            generated_tokens.append(token_id.item())
            
    # 4. è§£ç å¯¹æ¯”
    # çœŸå®å›¾
    z_q_true = vqvae.quantizer.embedding(target_tokens.unsqueeze(0)).permute(0, 3, 1, 2)
    img_true = vqvae.decoder(z_q_true)[0].cpu().permute(1, 2, 0).detach().numpy()
    
    # é¢„æµ‹å›¾
    gen_tensor = torch.tensor(generated_tokens).reshape(1, 16, 16).to(DEVICE)
    z_q_pred = vqvae.quantizer.embedding(gen_tensor).permute(0, 3, 1, 2)
    img_pred = vqvae.decoder(z_q_pred)[0].cpu().permute(1, 2, 0).detach().numpy()
    
    # æ‹¼å›¾
    img_true = np.clip(img_true, 0, 1) * 255
    img_pred = np.clip(img_pred, 0, 1) * 255
    res = np.hstack([img_true, img_pred])
    cv2.imwrite("debug_single_step.jpg", res)
    print("âœ… Prediction done. Check 'debug_single_step.jpg'.")
    print("â¬…ï¸ å·¦è¾¹: çœŸå®æœªæ¥ | â¡ï¸ å³è¾¹: GPTé¢„æµ‹æœªæ¥")

if __name__ == "__main__":
    test_teacher_forcing()