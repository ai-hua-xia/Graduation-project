import torch
import torch.nn.functional as F
import numpy as np
import cv2
import os
import time

# å¯¼å…¥ä½ çš„æ¨¡å‹å®šä¹‰
# å¦‚æœæŠ¥é”™è¯´æ‰¾ä¸åˆ°æ¨¡å—ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åä¸€è‡´
from train_vqvae_256 import VQVAE, IMAGE_SIZE
from train_world_model import WorldModelGPT, VOCAB_SIZE, TOKENS_PER_FRAME, BLOCK_SIZE

# ================= é…ç½® =================
# 1. æ¨¡å‹è·¯å¾„
VQVAE_PATH = "checkpoints_vqvae_256/vqvae_256_ep99.pth"
# è¿™é‡Œé€‰ä¸€ä¸ªä½ åˆšåˆšè®­ç»ƒå‡ºæ¥çš„æœ€æ–°æƒé‡ï¼Œæ¯”å¦‚ ep15, ep20 ç­‰
WORLD_MODEL_PATH = "checkpoints_world_model/world_model_ep99.pth" # ğŸ‘ˆ ä¿®æ”¹ä¸ºä½ ç°åœ¨çš„æœ€æ–°æ¨¡å‹

# 2. æ•°æ®è·¯å¾„ (ç”¨æ¥æå–ç¬¬ä¸€å¸§ä½œä¸ºç§å­)
DATA_PATH = "dataset_v2_complex/tokens_actions_vqvae_16x16.npz"

# 3. ç”Ÿæˆå‚æ•°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
STEPS_TO_DREAM = 100    # æƒ³è¦è®©å®ƒæƒ³è±¡å¤šå°‘å¸§ (æ¯”å¦‚ 100 å¸§)
TEMPERATURE = 0.3       # 0.8: ä¿å®ˆ/ç¨³å®š; 1.0: æ­£å¸¸; 1.2: æ›´æœ‰åˆ›é€ åŠ›ä½†ä¹Ÿæ›´å¯èƒ½å´©å
TOP_K = 10             # åªä»æ¦‚ç‡æœ€é«˜çš„ 100 ä¸ª token é‡Œé‡‡æ ·ï¼Œé˜²æ­¢ç”»é¢å‡ºç°ä¹±ç 

OUTPUT_VIDEO = "dream_result.mp4"
# =======================================

def load_models():
    print("â³ Loading VQ-VAE...")
    vqvae = VQVAE().to(DEVICE)
    vqvae.load_state_dict(torch.load(VQVAE_PATH, map_location=DEVICE)["model"])
    vqvae.eval()

    print(f"â³ Loading World Model from {WORLD_MODEL_PATH}...")
    gpt = WorldModelGPT().to(DEVICE)
    checkpoint = torch.load(WORLD_MODEL_PATH, map_location=DEVICE)
    gpt.load_state_dict(checkpoint["model"])
    gpt.eval()
    return vqvae, gpt

def decode_indices(vqvae, indices):
    """æŠŠ (16, 16) çš„ token çŸ©é˜µè¿˜åŸæˆå›¾ç‰‡"""
    with torch.no_grad():
        # indices shape: (16, 16) -> (1, 16, 16)
        indices_tensor = torch.LongTensor(indices).unsqueeze(0).to(DEVICE)
        # VQVAE çš„ decode_indices éœ€è¦ indices å·²ç»æ˜¯ Embedding åçš„è¿˜æ˜¯ç›´æ¥ indices?
        # æŸ¥çœ‹ä¹‹å‰çš„ VQVAE ä»£ç ï¼Œé€šå¸¸éœ€è¦é€šè¿‡ quantizer æŸ¥è¡¨ã€‚
        # ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ quantizer çš„ embedding æŸ¥è¡¨åŠŸèƒ½
        
        # 1. æŸ¥è¡¨è·å– quant vectors
        z_q = vqvae.quantizer.embedding(indices_tensor) # (1, 16, 16, 64)
        z_q = z_q.permute(0, 3, 1, 2) # (1, 64, 16, 16)
        
        # 2. è§£ç 
        decoded_img = vqvae.decoder(z_q)
        
        # 3. è½¬å› numpy å›¾ç‰‡æ ¼å¼
        img = decoded_img[0].cpu().permute(1, 2, 0).numpy()
        img = np.clip(img, 0, 1) * 255
        return img.astype(np.uint8)

def sample_next_token(logits, temperature=1.0, top_k=None):
    """ä»é¢„æµ‹ç»“æœä¸­é‡‡æ ·"""
    logits = logits[:, -1, :] / temperature # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
    if top_k is not None:
        v, _ = torch.topk(logits, top_k)
        logits[logits < v[:, [-1]]] = -float('Inf')
    
    probs = F.softmax(logits, dim=-1)
    idx = torch.multinomial(probs, num_samples=1)
    return idx

def main():
    vqvae, gpt = load_models()
    
    # 1. åŠ è½½çœŸå®æ•°æ®ä½œä¸ºâ€œç§å­â€
    print("ğŸŒ± Loading Seed Data...")
    data = np.load(DATA_PATH)
    all_tokens = data['tokens']   # (N, 16, 16)
    all_actions = data['actions'] # (N, 2)
    
    # æˆ‘ä»¬ä»ç¬¬ 500 å¸§å¼€å§‹ï¼Œä½œä¸ºèµ·å§‹çŠ¶æ€
    start_idx = 500
    
    # ================= ğŸ”´ æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ =================
    # åœ¨è½¬ä¸º Tensor åï¼Œå¿…é¡»åŠ ä¸Š .long()ï¼ŒæŠŠ uint16 å¼ºè½¬ä¸º int64
    context_tokens = torch.from_numpy(all_tokens[start_idx].reshape(1, -1)).long().to(DEVICE) 
    # ===================================================
    
    context_tokens = context_tokens.unsqueeze(0) # (1, 1, 256) -> batch=1, seq=1, dim=256
    
    # æå–æœªæ¥ 100 å¸§çš„çœŸå®åŠ¨ä½œ
    future_actions = torch.from_numpy(all_actions[start_idx:start_idx + STEPS_TO_DREAM]).float().to(DEVICE)
    future_actions = future_actions.unsqueeze(0) # (1, STEPS, 2)
    
    # ç”¨äºä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
    generated_frames = []
    
    # å…ˆæŠŠç¬¬ä¸€å¸§è§£ç å‡ºæ¥å­˜ç€
    first_frame = decode_indices(vqvae, all_tokens[start_idx])
    generated_frames.append(cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR))
    
    print(f"ğŸš€ Dreaming start! Context window: {BLOCK_SIZE} tokens")
    
    with torch.no_grad():
        current_tokens = context_tokens # (1, seq_len, 256)
        current_actions = future_actions[:, 0:1, :] # å–ç¬¬ä¸€ä¸ªåŠ¨ä½œ (1, 1, 2)
        
        for step in range(STEPS_TO_DREAM - 1):
            t0 = time.time()
            
            # å‡†å¤‡å½“å‰è¦ç”Ÿæˆçš„å¸§çš„å®¹å™¨
            next_frame_tokens = []
            
            # è·å–å½“å‰ä¸Šä¸‹æ–‡çš„åŠ¨ä½œ
            this_step_action = future_actions[:, step:step+1, :] # (1, 1, 2)
            
            # è¿™é‡Œçš„æ»‘åŠ¨çª—å£é€»è¾‘
            MAX_CONTEXT_FRAMES = 3
            if current_tokens.shape[1] > MAX_CONTEXT_FRAMES:
                current_tokens = current_tokens[:, -MAX_CONTEXT_FRAMES:, :]
                current_actions = current_actions[:, -MAX_CONTEXT_FRAMES:, :]
            
            # åŸºç¡€è¾“å…¥æ„é€ 
            pred_tokens_so_far = torch.zeros((1, 1, 256), dtype=torch.long).to(DEVICE)
            
            # æ‹¼æ¥ Img å’Œ Act
            # æ­¤æ—¶ current_tokens å·²ç»æ˜¯ long ç±»å‹ï¼Œpred_tokens_so_far ä¹Ÿæ˜¯ long ç±»å‹ï¼Œä¸ä¼šæŠ¥é”™äº†
            full_input_tokens = torch.cat([current_tokens, pred_tokens_so_far], dim=1) 
            full_input_actions = torch.cat([current_actions, this_step_action], dim=1) 
            
            for i in range(256):
                logits, _ = gpt(full_input_tokens, full_input_actions)
                
                seq_len = current_tokens.shape[1] 
                target_idx = seq_len * 257 + i - 1
                
                # å®‰å…¨æ£€æŸ¥
                if target_idx >= logits.shape[1]:
                    target_idx = logits.shape[1] - 1
                    
                next_token_logits = logits[:, target_idx, :]
                
                # é‡‡æ ·
                idx = sample_next_token(next_token_logits.unsqueeze(1), temperature=TEMPERATURE, top_k=TOP_K)
                
                # å¡«å…¥ tensor
                full_input_tokens[0, -1, i] = idx
            
            # ä¸€å¸§ç”Ÿæˆå®Œæ¯•ï¼
            new_frame_tokens = full_input_tokens[:, -1:, :] # (1, 1, 256)
            
            # è§£ç æ˜¾ç¤º
            # æ³¨æ„ï¼šdecode_indices éœ€è¦ numpy æ ¼å¼
            img_np = decode_indices(vqvae, new_frame_tokens.reshape(16, 16).cpu().numpy())
            generated_frames.append(cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            current_tokens = torch.cat([current_tokens, new_frame_tokens], dim=1)
            current_actions = torch.cat([current_actions, this_step_action], dim=1)
            
            print(f"Frame {step+1}/{STEPS_TO_DREAM} generated. Time: {time.time()-t0:.2f}s")

    # ä¿å­˜è§†é¢‘
    print("ğŸ’¾ Saving video (step 1: raw export)...")
    height, width, layers = generated_frames[0].shape
    
    # 1. å…ˆä¿å­˜ä¸ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ (ä½¿ç”¨ mp4vï¼Œå› ä¸ºè¿™æ˜¯ OpenCV æ”¯æŒæœ€ç¨³çš„ï¼Œä¸å®¹æ˜“æŠ¥é”™)
    temp_output = "temp_dream_raw.mp4"
    video = cv2.VideoWriter(temp_output, cv2.VideoWriter_fourcc(*'mp4v'), 10, (width, height))
    
    for frame in generated_frames:
        video.write(frame)
    video.release()
    
    # 2. è°ƒç”¨ FFmpeg è‡ªåŠ¨è½¬ç  (è½¬æˆ VS Code èƒ½æ’­çš„ H.264 æ ¼å¼)
    # æ³¨æ„ï¼šè¿™éœ€è¦ä½ çš„æœåŠ¡å™¨ä¸Šå®‰è£…äº† ffmpeg (é€šå¸¸åšæ·±åº¦å­¦ä¹ ç¯å¢ƒéƒ½æœ‰)
    print("âš™ï¸ Auto-converting to H.264 for VS Code compatibility...")
    
    # -y: è¦†ç›–åŒåæ–‡ä»¶
    # -vcodec libx264: ä½¿ç”¨ H.264 ç¼–ç 
    # -pix_fmt yuv420p: ç¡®ä¿æµè§ˆå™¨/VSCode å…¼å®¹æ€§
    # -loglevel error: å°‘è¾“å‡ºåºŸè¯
    convert_cmd = f"ffmpeg -y -i {temp_output} -vcodec libx264 -pix_fmt yuv420p -loglevel error {OUTPUT_VIDEO}"
    
    exit_code = os.system(convert_cmd)
    
    if exit_code == 0:
        # è½¬ç æˆåŠŸï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_output):
            os.remove(temp_output)
        print(f"âœ… Dream video saved to {OUTPUT_VIDEO} (VS Code å¯ç›´æ¥æ’­æ”¾)")
    else:
        # è½¬ç å¤±è´¥ï¼ˆå¯èƒ½æ²¡è£… ffmpegï¼‰ï¼Œä¿ç•™åŸæ–‡ä»¶
        print(f"âš ï¸ è½¬ç å¤±è´¥ (å¯èƒ½æœªå®‰è£… ffmpeg)ï¼Œè¯·ä¸‹è½½ {temp_output} åˆ°æœ¬åœ°æ’­æ”¾ã€‚")

if __name__ == "__main__":
    main()