import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import math
import time

# ================= é…ç½®åŒºåŸŸ =================
DATA_PATH = "dataset_v2_complex/tokens_actions_vqvae_16x16.npz"
OUT_DIR = "checkpoints_world_model"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æ¨¡å‹å‚æ•°
VOCAB_SIZE = 1024       # VQ-VAE çš„è¯è¡¨å¤§å°
ACTION_DIM = 2          # åŠ¨ä½œç»´åº¦ (è½¬å‘, æ²¹é—¨)
N_EMBD = 512            # åµŒå…¥ç»´åº¦ (GPTçš„éšè—å±‚å¤§å°)
N_HEAD = 8              # æ³¨æ„åŠ›å¤´æ•°
N_LAYER = 8             # Transformer å±‚æ•°
DROPOUT = 0.1

# åºåˆ—å‚æ•°
TOKENS_PER_FRAME = 256  # 16x16
BLOCK_SIZE = 257 * 4    # ä¸Šä¸‹æ–‡é•¿åº¦ï¼šçœ‹è¿‡å» 4 å¸§ (256å›¾ + 1åŠ¨ä½œ) * 4
BATCH_SIZE = 16         # æ˜¾å­˜ä¸å¤Ÿå°±æ”¹å°ï¼Œæ¯”å¦‚ 8 æˆ– 4
LEARNING_RATE = 3e-4
MAX_EPOCHS = 100
SAVE_EVERY = 5          # æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡

os.makedirs(OUT_DIR, exist_ok=True)

# ================= 1. æ•°æ®é›†å®šä¹‰ =================
class WorldModelDataset(Dataset):
    def __init__(self, data_path, seq_len=4):
        print(f"Loading data from {data_path}...")
        data = np.load(data_path)
        self.tokens = data['tokens']   # (N, 16, 16)
        self.actions = data['actions'] # (N, 2)
        self.indices = data['indices'] # (N,) ç”¨äºåˆ¤æ–­æ˜¯å¦è¿ç»­
        
        # å±•å¹³ Token: (N, 16, 16) -> (N, 256)
        self.n_samples = len(self.tokens)
        self.tokens_flat = self.tokens.reshape(self.n_samples, -1).astype(np.int64)
        
        self.seq_len = seq_len # ä¸€æ¬¡æ‹¿å‡ å¸§è®­ç»ƒ
        self.frame_struct_len = TOKENS_PER_FRAME + 1 # ä¸€å¸§çš„æ€»é•¿åº¦ (256å›¾ + 1åŠ¨ä½œ)

        # é¢„è®¡ç®—æ‰€æœ‰æœ‰æ•ˆçš„èµ·å§‹ç´¢å¼•ï¼ˆé˜²æ­¢è·¨è§†é¢‘é‡‡æ ·ï¼‰
        self.valid_starts = []
        for i in range(self.n_samples - self.seq_len):
            # æ£€æŸ¥è¿™å‡ å¸§åœ¨åŸå§‹è§†é¢‘é‡Œæ˜¯å¦æ˜¯è¿ç»­çš„ (index å¿…é¡»è¿å·)
            # ä¾‹å¦‚: indices[i+seq_len] - indices[i] åº”è¯¥ç­‰äº seq_len
            if self.indices[i + self.seq_len] - self.indices[i] == self.seq_len:
                self.valid_starts.append(i)
        
        print(f"Data loaded. Total frames: {self.n_samples}. Valid sequences: {len(self.valid_starts)}")

    def __len__(self):
        return len(self.valid_starts)

    def __getitem__(self, idx):
        # è·å–è¿™ä¸€æ®µçš„èµ·å§‹å¸§ç´¢å¼•
        start_idx = self.valid_starts[idx]
        end_idx = start_idx + self.seq_len
        
        # æå–æ•°æ®æ®µ
        batch_tokens = self.tokens_flat[start_idx:end_idx] # (seq_len, 256)
        batch_actions = self.actions[start_idx:end_idx]    # (seq_len, 2)
        
        # æ„é€ è¾“å…¥åºåˆ—ï¼š [Img0, Act0, Img1, Act1, ...]
        # æˆ‘ä»¬éœ€è¦æŠŠ Image Token å’Œ Action æ‹¼èµ·æ¥ã€‚
        # ä¸ºäº†æ–¹ä¾¿å¤„ç†ï¼Œæˆ‘ä»¬åªè¿”å›åŸå§‹æ•°æ®ï¼Œåœ¨ collate_fn æˆ– forward é‡Œå†æ‹¼æ¥ embedding
        
        return {
            "tokens": torch.from_numpy(batch_tokens),
            "actions": torch.from_numpy(batch_actions).float()
        }

# ================= 2. GPT æ¨¡å‹å®šä¹‰ =================
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert N_EMBD % N_HEAD == 0
        self.c_attn = nn.Linear(N_EMBD, 3 * N_EMBD)
        self.c_proj = nn.Linear(N_EMBD, N_EMBD)
        self.attn_dropout = nn.Dropout(DROPOUT)
        self.resid_dropout = nn.Dropout(DROPOUT)
        self.n_head = N_HEAD
        self.n_embd = N_EMBD
        # å› æœé®ç½© (Mask)
        self.register_buffer("bias", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))
                                     .view(1, 1, BLOCK_SIZE, BLOCK_SIZE))

    def forward(self, x):
        B, T, C = x.size()
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.resid_dropout(self.c_proj(y))

class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.LayerNorm(N_EMBD)
        self.attn = CausalSelfAttention(config)
        self.ln2 = nn.LayerNorm(N_EMBD)
        self.mlp = nn.Sequential(
            nn.Linear(N_EMBD, 4 * N_EMBD),
            nn.GELU(),
            nn.Linear(4 * N_EMBD, N_EMBD),
            nn.Dropout(DROPOUT),
        )

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class WorldModelGPT(nn.Module):
    def __init__(self):
        super().__init__()
        # 1. åµŒå…¥å±‚
        self.token_embedding = nn.Embedding(VOCAB_SIZE, N_EMBD)
        self.action_embedding = nn.Linear(ACTION_DIM, N_EMBD) # è¿ç»­åŠ¨ä½œæ˜ å°„åˆ° embedding ç©ºé—´
        self.position_embedding = nn.Embedding(BLOCK_SIZE, N_EMBD)
        
        # 2. Transformer Blocks
        self.blocks = nn.Sequential(*[Block(None) for _ in range(N_LAYER)])
        self.ln_f = nn.LayerNorm(N_EMBD)
        
        # 3. è¾“å‡ºå¤´ (é¢„æµ‹ä¸‹ä¸€ä¸ª Token)
        self.head = nn.Linear(N_EMBD, VOCAB_SIZE, bias=False)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def forward(self, token_seq, action_seq, targets=None):
        """
        token_seq: (B, seq_len, 256)
        action_seq: (B, seq_len, 2)
        """
        B, seq_len, _ = token_seq.shape
        
        # --- æ ¸å¿ƒé€»è¾‘ï¼šæ„é€ è¾“å…¥åºåˆ— ---
        # æ¯ä¸€å¸§å˜æˆäº† 257 ä¸ª token: [256ä¸ªå›¾token, 1ä¸ªåŠ¨ä½œembedding]
        # æ€»é•¿åº¦ T = seq_len * 257
        
        # 1. æŠŠ Image Tokens å˜æˆ Embedding
        img_embs = self.token_embedding(token_seq) # (B, seq_len, 256, N_EMBD)
        
        # 2. æŠŠ Action å˜æˆ Embedding
        act_embs = self.action_embedding(action_seq) # (B, seq_len, N_EMBD)
        act_embs = act_embs.unsqueeze(2) # (B, seq_len, 1, N_EMBD)
        
        # 3. æ‹¼æ¥: åœ¨æ¯å¸§çš„ 256 ä¸ªå›¾ token åé¢æ‹¼ 1 ä¸ªåŠ¨ä½œ token
        # å½¢çŠ¶å˜: (B, seq_len, 257, N_EMBD)
        x = torch.cat([img_embs, act_embs], dim=2) 
        
        # 4. å±•å¹³ä¸ºæ—¶é—´åºåˆ—
        # å½¢çŠ¶å˜: (B, seq_len * 257, N_EMBD) -> (B, T, N_EMBD)
        x = x.view(B, -1, N_EMBD)
        
        # 5. åŠ ä¸Šä½ç½®ç¼–ç 
        T = x.size(1)
        if T > BLOCK_SIZE:
             # å¦‚æœåºåˆ—å¤ªé•¿ï¼ˆæ¯”å¦‚ç¬¬ä¸€æ¬¡è¿è¡Œï¼‰ï¼Œæˆªæ–­ï¼ˆè™½ç„¶ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼‰
             x = x[:, :BLOCK_SIZE, :]
             T = BLOCK_SIZE
             
        pos_idxs = torch.arange(T, device=x.device)
        pos_emb = self.position_embedding(pos_idxs)
        x = x + pos_emb
        
        # 6. Transformer Forward
        x = self.blocks(x)
        x = self.ln_f(x)
        
        # 7. è®¡ç®— Loss
        logits = self.head(x) # (B, T, VOCAB_SIZE)
        
        loss = None
        if targets is not None:
            # targets çš„æ„é€ éœ€è¦ç¨å¾®è´¹ç‚¹åŠ²
            # æˆ‘ä»¬çš„ x æ˜¯: [I0...I0, A0, I1...I1, A1, ...]
            # æˆ‘ä»¬å¸Œæœ›é¢„æµ‹: [I0...I0, I1...I1, A1...] çš„ä¸‹ä¸€ä¸ª
            # å…¶å®æœ€ç®€å•çš„è‡ªå›å½’ç›®æ ‡æ˜¯ï¼šè¾“å…¥ idx çš„é¢„æµ‹ç›®æ ‡æ˜¯ idx+1
            
            # æ„é€ å®Œæ•´çš„ target åºåˆ—ç´¢å¼•
            # Image tokens: 0~1023
            # Action ä½ç½®æˆ‘ä»¬ä¸æƒ³è®¡ç®— Loss (å› ä¸ºåŠ¨ä½œæ˜¯è¿ç»­å€¼ï¼Œä¸”æ˜¯ç»™å®šçš„æ¡ä»¶ï¼Œä¸æ˜¯é¢„æµ‹ç›®æ ‡)
            # æ‰€ä»¥æˆ‘ä»¬åœ¨ target é‡ŒæŠŠ Action çš„ä½ç½®è®¾ä¸º -1 (ignore_index)
            
            # å‡†å¤‡ Target Tensor
            # åŸå§‹ targets æ˜¯è¾“å…¥çš„ token_seqï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬æŒ‰é¡ºåºæ’å¥½
            # (B, seq_len, 256) -> (B, seq_len * 257) ? ä¸å¯¹ï¼Œè¿™é‡Œåªæœ‰256ä¸ª
            
            flat_tokens = token_seq.view(B, -1) # (B, seq_len * 256)
            # æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ªå’Œ x ä¸€æ ·é•¿çš„ (B, seq_len * 257) çš„ target çŸ©é˜µ
            # å…¶ä¸­ Image ä½ç½®å¡« Image Tokenï¼ŒAction ä½ç½®å¡« -1
            
            target_seq = torch.full((B, seq_len, 257), -1, dtype=torch.long, device=DEVICE)
            target_seq[:, :, :256] = token_seq
            target_seq = target_seq.view(B, -1) # (B, T)

            # Shift predict:
            # logitsé¢„æµ‹çš„æ˜¯ä¸‹ä¸€ä¸ªè¯ã€‚æ‰€ä»¥ logits[:, :-1] åº”è¯¥é¢„æµ‹ target_seq[:, 1:]
            
            logits = logits[:, :-1, :]
            target_seq = target_seq[:, 1:]
            
            # Flatten for loss
            loss = F.cross_entropy(logits.reshape(-1, VOCAB_SIZE), target_seq.reshape(-1), ignore_index=-1)

        return logits, loss

# ================= 3. è®­ç»ƒä¸»å¾ªç¯ =================
def main():
    # 1. å‡†å¤‡æ•°æ®
    dataset = WorldModelDataset(DATA_PATH, seq_len=int(BLOCK_SIZE/257))
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = WorldModelGPT().to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # 3. å°è¯•åŠ è½½æ–­ç‚¹
    start_epoch = 0
    checkpoints = sorted([f for f in os.listdir(OUT_DIR) if f.endswith(".pth")])
    if checkpoints:
        latest = os.path.join(OUT_DIR, checkpoints[-1])
        print(f"ğŸ”„ Resuming from {latest}")
        ckpt = torch.load(latest, map_location=DEVICE)
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        start_epoch = ckpt['epoch'] + 1

    # 4. å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ Start Training World Model on {DEVICE}...")
    model.train()
    
    for epoch in range(start_epoch, MAX_EPOCHS):
        total_loss = 0
        start_time = time.time()
        
        for i, batch in enumerate(dataloader):
            tokens = batch['tokens'].to(DEVICE)   # (B, seq, 256)
            actions = batch['actions'].to(DEVICE) # (B, seq, 2)
            
            optimizer.zero_grad()
            
            # Forward (ä¼ å…¥ tokens ä½œä¸º target)
            _, loss = model(tokens, actions, targets=tokens)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            optimizer.step()
            
            total_loss += loss.item()
            
            if i % 10 == 0:
                print(f"Epoch {epoch} | Step {i}/{len(dataloader)} | Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(dataloader)
        print(f"âœ… Epoch {epoch} Done. Avg Loss: {avg_loss:.4f}. Time: {time.time()-start_time:.1f}s")
        
        # ä¿å­˜æ¨¡å‹
        if epoch % SAVE_EVERY == 0 or epoch == MAX_EPOCHS - 1:
            save_path = os.path.join(OUT_DIR, f"world_model_ep{epoch}.pth")
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'loss': avg_loss
            }, save_path)
            print(f"ğŸ’¾ Saved checkpoint: {save_path}")

if __name__ == "__main__":
    main()